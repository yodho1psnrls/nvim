-- https://github.com/huggingface/llm.nvim
-- https://github.com/ollama/ollama/blob/main/docs/api.md
-- https://github.com/nomnivore/ollama.nvim

return {
  {
    'huggingface/llm.nvim',
    opts = {
			api_token = nil, -- cf Install paragraph
			model = "deepseek-coder:1.3b", -- the model ID, behavior depends on backend
			backend = "ollama", -- backend ID, "huggingface" | "ollama" | "openai" | "tgi"
			-- url = nil, -- the http url of the backend
			-- url = "http://localhost:11434/v1", -- llm-ls uses "/api/generate"
			url = "http://localhost:11434/api/generate", -- llm-ls uses "/api/generate"
			-- tokens_to_clear = { "<|endoftext|>" }, -- tokens to remove from the model's output
			tokens_to_clear = { "<|EOT|>" }, -- tokens to remove from the model's output
			-- parameters that are added to the request body, values are arbitrary, you can set any field:value pair here it will be passed as is to the backend
			request_body = {
				options = {
					temperature = 0.2,
					top_p = 0.95,
				},
				--[[parameters = {
					max_new_tokens = 60,
					temperature = 0.2,
					top_p = 0.95,
				},]]--
			},
			-- set this if the model supports fill in the middle
			--[[fim = {
				enabled = true,
				prefix = "<fim_prefix>",
				middle = "<fim_middle>",
				suffix = "<fim_suffix>",
			},]]--
			debounce_ms = 150,
			accept_keymap = "<Tab>",
			dismiss_keymap = "<S-Tab>",
			tls_skip_verify_insecure = false,
			-- llm-ls configuration, cf llm-ls section
			lsp = {
				-- bin_path = "D:\\Program Files\\llm-ls\\llm-ls.exe",
				bin_path = 'llm-ls',
				host = 'localhost',
				-- host = '127.0.0.1',
				port = '11434',
				-- cmd_env = nil, -- or { LLM_LOG_LEVEL = "DEBUG" } to set the log level of llm-ls
				-- cmd_env = { LLM_LOG_LEVEL = "DEBUG" },
				cmd_env = { "llm-ls", "--port", "11434" },
				version = "0.5.2",
			},
			tokenizer = nil, -- cf Tokenizer paragraph
			context_window = 1024, -- max number of tokens for the context window
			enable_suggestions_on_startup = true,
			enable_suggestions_on_files = "*", -- pattern matching syntax to enable suggestions on specific files, either a string or a list of strings
			disable_url_path_completion = false, -- cf Backend
    },
		config = function (_, opts)
			require('llm').setup(opts)
			-- print(vim.api.nvim_call_function("stdpath", { "data" }) .. "\\llm_nvim\\bin")
		end
  },
}
